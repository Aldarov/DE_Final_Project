'''Проект создания ETL-процесса для анализа публикуемых новостей.'''

Проект создает ETL-процесс формирования витрины данных для анализа публикаций новостей. В качестве источников данных используются rss-ленты новостей следующих ресурсов: [https://lenta.ru/rss ''https://lenta.ru/rss''], [https://www.vedomosti.ru/rss/news ''https://www.vedomosti.ru/rss/news''], [https://tass.ru/rss/v2.xml ''https://tass.ru/rss/v2.xml'']

Итоговая витрина данных содержит следующие данные:

* Суррогатный ключ категории
* Название категории
* Общее количество новостей из всех источников по данной категории за все время
* Количество новостей данной категории для каждого из источников за все время
* Общее количество новостей из всех источников по данной категории за последние сутки
* Количество новостей данной категории для каждого из источников за последние сутки
* Среднее количество публикаций по данной категории в сутки
* День, в который было сделано максимальное количество публикаций по данной новости
* Количество публикаций новостей данной категории по дням недели

Т.к. в разных источниках категории новостей отличаться, был проведен их анализ, в результате выделены обобщенные категории по всем источникам.

'''Стек технологий.'''

Т.к. источники данных предоставляют структурированный набор данных в rss-формате небольшого размера, в качестве базы данных используется PostgreSql.

Для обработки данных применяется язык python, со следующими библиотеками:

* psycopg2 – используется для работы с PostgreSql
* requests – используется для работы с HTTP-запросами
* beautifulsoup4, lxml – используется для парсинга xml (rss-формата)
* python-dotenv – используется для обработки .env файла

Для оркестрации процесса обработки данных используется - Apache Airflow

Для автоматизации создания, управления и развертывания приложения используется – Docker

'''Реализация.'''

Структуру хранения данных состоит из трех слоев:

* Сырой слой данных
* Промежуточный слой
* Слой витрин

ER-диаграмма БД:

[[File:docs/erd.jpg|623x433px]]

# Сырые данные из источников сохраняются в таблицу raw_data без изменений.
# Перед каждой загрузкой, таблица raw_data очищается.
# Перед загрузкой в сырой слой, все данные валидируются на корректность, все некорректные данные помещаются в таблицу logs.
# Данные обрабатываются на основе таблицы categories_relationship, в которой лежат данные взаимосвязи итоговых категорий новостей, c категориями из разных источников данных. Все обработанные данные сохраняются в таблицу processed_data. Если таблица processed_data еще пустая в нее добавляются все данные из raw_data, иначе только новые данные, которые ранее не были добавлены.
# На основе данных из processed_data, создается итоговая витрина данных, которая сохраняется в таблицу news_by_category_showcase
# Apache Airflow последовательно запускает все этапы один раз в сутки

Все источники данных хранятся в таблицы sources, в нее можно добавлять дополнительные источники.


'''Для запуска системы:'''

# Перед запуском в корне проекта создать файл .env с параметрами, описанными в файле .env.example.
# Проинициализировать airflow командой: docker compose up airflow-init
# Запускаем контейнеры: docker compose up –d
# Для входа в airflow используем url: http://localhost:8080