# Проект создания ETL-процесса формирования витрин данных для анализа публикаций новостей.

Проект создает ETL-процесс формирования витрины данных для анализа публикаций новостей. В качестве источников данных используются rss-ленты новостей следующих ресурсов: https://lenta.ru/rss, https://www.vedomosti.ru/rss/news, https://tass.ru/rss/v2.xml

Итоговая витрина данных содержит следующие данные:
* Суррогатный ключ категории
* Название категории
* Общее количество новостей из всех источников по данной категории за все время
* Количество новостей данной категории для каждого из источников за все время
* Общее количество новостей из всех источников по данной категории за последние сутки
* Количество новостей данной категории для каждого из источников за последние сутки
* Среднее количество публикаций по данной категории в сутки
* День, в который было сделано максимальное количество публикаций по данной новости
* Количество публикаций новостей данной категории по дням недели

Т.к. в разных источниках категории новостей отличаются, был проведен их анализ, в результате выделены обобщенные категории по всем источникам.

## Стек технологий:

Т.к. источники данных предоставляют структурированный набор данных в rss-формате небольшого размера, в качестве базы данных используется **PostgreSql**.

Для обработки данных применяется язык **Python**, со следующими библиотеками:

* **psycopg2** – используется для работы с PostgreSql
* **requests** – используется для работы с HTTP-запросами
* **beautifulsoup4, lxml** – используется для парсинга xml (rss-формата)
* **python-dotenv** – используется для обработки .env файла

Для оркестрации процесса обработки данных используется - **Apache Airflow**

Для автоматизации создания, управления и развертывания приложения используется – **Docker**

## Реализация:

Структуру хранения данных состоит из трех слоев:
* Сырой слой данных
* Промежуточный слой
* Слой витрин

ER-диаграмма БД:

![ER-диаграмма БД](/docs/erd.jpg "ER-диаграмма БД")

1. Сырые данные из источников сохраняются в таблицу raw_data без изменений.
2. Перед каждой загрузкой, таблица raw_data очищается.
3. Перед загрузкой в сырой слой, все данные валидируются на корректность, все некорректные данные помещаются в таблицу logs.
4. Данные обрабатываются на основе таблицы categories_relationship, в которой лежат данные взаимосвязи итоговых категорий новостей, c категориями из разных источников данных. Все обработанные данные сохраняются в таблицу processed_data. Если таблица processed_data еще пустая в нее добавляются все данные из raw_data, иначе только новые данные, которые ранее не были добавлены.
5. На основе данных из processed_data, создается итоговая витрина данных, которая сохраняется в таблицу news_by_category_showcase
6. Apache Airflow последовательно запускает все этапы один раз в сутки

Все источники данных хранятся в таблицы sources, в нее можно добавлять дополнительные источники.


## Для запуска системы:

1. Перед запуском в корне проекта создать файл **.env** с параметрами, описанными в файле **.env.example**.
2. Проинициализировать airflow командой: **docker compose up airflow-init**
3. Запускаем контейнеры: **docker compose up –d**
4. Для входа в airflow используем url: **http://localhost:8080**
